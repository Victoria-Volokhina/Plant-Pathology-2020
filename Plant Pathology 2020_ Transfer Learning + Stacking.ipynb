{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle. Plant Pathology 2020. Transfer Learning + Stacking\n**Identify the category of foliar diseases in apple trees**"},{"metadata":{},"cell_type":"markdown","source":"# Contents\n* [<font size=4>Plan</font>](#1)\n* [<font size=4>Results</font>](#2)\n* [<font size=4>Preparing the ground</font>](#3)\n    * [TPU Config](#3.1)\n    * [Load Labels](#3.2)\n    * [Visualize one leaf](#3.3)\n    * [Image processing and augmentation](#3.4)\n    * [Visualize results](#3.5)\n    * [Save predictions](#3.6)\n    * [Learning Rate](#3.7)\n* [<font size=4>Modeling</font>](#4)\n    * [EfficientNetB7](#4.1)\n    * [ResNet50V2](#4.2)\n    * [InceptionResNetV2](#4.3)\n    * [InceptionV3](#4.4)\n    * [Xception](#4.5)\n    * [ResNet152V2](#4.6)\n    * [NASNetLarge](#4.7)\n    * [DenseNet201](#4.8)\n* [<font size=4>Stacking</font>](#5)\n    * [Avg](#5.1)\n    * [Ridge meta model](#5.2)\n    * [MLPClassifier meta model](#5.3)\n    * [Entropy](#5.4)\n"},{"metadata":{},"cell_type":"markdown","source":"# Plan <a id=\"1\"></a>\n1. Train/val 0.8/0.2. Choose the 3 best models with val_acc.\n2. Train/val 0.85/0.15. Train the 3 best models.\n3. Stack predictions for the 3 best models for validation dataset.\n4. Stack predictions for the 3 best models for test dataset.\n4. Train meta model with stacking predictions for validation dataset.\n5. Get predictions with meta model for test dataset.\n6. Compare with the average predictions of the 3 best models"},{"metadata":{},"cell_type":"markdown","source":"# Results  <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"1. EfficientNetB7: acc: 0.000, val_acc: 0.000, LB: 0.977 (0.972)\n2. ResNet50V2:  60 epoch, 1024: acc: 0.9815, val_acc: 0.9644 \n3. ResNet50: 100 epoch: acc: 0.92542, val_acc: 0.9233    \n4. IncResNetV2: 100 epoch: acc: 0.9908, val_acc: 0.9534\n5. InceptionV3: 40 epoch: acc: 0.9972, val_acc: 0.9562 \n6. Xception: 40 epochs: acc: 0.9993, val_acc: 0.9726, LB: 0.954    \n7. ResNet152V2: 40 epochs: acc: 0.9993, val_acc: 0.9562\n8. Avg  IncV3, Xcept, ResNet152V2, 40 epoch LB: 0.948 "},{"metadata":{},"cell_type":"markdown","source":"100 epoch, 16*, Val 0.15, img 800\n\n1. Xcept: loss: 0.0032 - categorical_accuracy: 0.9993 - val_loss: 0.0465 - val_categorical_accuracy: 0.9818, LB: 0.971\n2. ResNet50V2: loss: 0.0119 - categorical_accuracy: 0.9974 - val_loss: 0.1597 - val_categorical_accuracy: 0.9453\n3. InceptionV3: loss: 0.0018 - categorical_accuracy: 0.9993 - val_loss: 0.0475 - val_categorical_accuracy: 0.9635\n4. EffNetB7: loss: 0.0041 - categorical_accuracy: 0.9948 - val_loss: 0.1023 - val_categorical_accuracy: 0.9714\n5. NasNet: loss: 0.0011 - categorical_accuracy: 0.9993 - val_loss: 0.4238 - val_categorical_accuracy: 0.9343"},{"metadata":{},"cell_type":"markdown","source":"100 epoch, 16*, Val 0.15, img 533-800, Avg. LB 0.971\n1. InceptionV3: loss: 0.0079 - categorical_accuracy: 0.9974 - val_loss: 0.1028 - val_categorical_accuracy: 0.9745\n2. Xcept: loss: 0.0033 - categorical_accuracy: 0.9987 - val_loss: 0.0774 - val_categorical_accuracy: 0.9635 \n3. EffNetB7 30 epoch: loss: 0.0066 - categorical_accuracy: 0.9974 - val_loss: 0.0755 - val_categorical_accuracy: 0.9745\n4. ResNet152V2: val_categorical_accuracy: 0.9635\n5. DenseNet201: loss: 0.0039 - categorical_accuracy: 0.9987 - val_loss: 0.0451 - val_categorical_accuracy: 0.9854, LB 0.963"},{"metadata":{},"cell_type":"markdown","source":"100 epoch, 16*, no Val, img 533-800\n1. Entropy. InceptionV3 + DenseNet201 + EffNetB7. LB entr: 0.969, 0.970\n2. Avg. InceptionV3 + DenseNet201 + EffNetB7. LB avg: 0.974\n3. 5 models entropy: 0.970\n4. 5 models avg. LB: 0.972"},{"metadata":{},"cell_type":"markdown","source":"# Preparing the ground  <a id=\"3\"></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.layers as L\nfrom keras.models import Model\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TPU Config  <a id=\"3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\ndef seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 2048\nseed_everything(seed)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\n#BATCH_SIZE = 8 * strategy.num_replicas_in_sync\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nEPOCHS = 100\nimage_size1 = 800\n#image_size1 = 533\nimage_size2 = 800","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Labels  <a id=\"3.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_path(st):\n    return GCS_DS_PATH + '/images/' + st + '.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\nsub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\ntrain_labels = train.loc[:, 'healthy':].values\n\nvalid_dataset = []\nSPLIT_VALIDATION = True\nif SPLIT_VALIDATION:\n    train_paths, valid_paths, train_labels, valid_labels =train_test_split(train_paths, train_labels, test_size=0.15, random_state=24)\n    valid_labels_df = pd.DataFrame({'healthy': valid_labels[:, 0], \n                                  'multiple_diseases': valid_labels[:, 1], \n                                  'rust': valid_labels[:, 2], \n                                  'scab': valid_labels[:, 3]})\n    valid_labels_df.to_csv('valid_labels.csv', index=False)\n    \ntrain_labels_df = pd.DataFrame({'healthy': train_labels[:, 0], \n                              'multiple_diseases': train_labels[:, 1], \n                              'rust': train_labels[:, 2], \n                              'scab': train_labels[:, 3]})\ntrain_labels_df.to_csv('train_labels.csv', index=False)\n    \nSTEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize one leaf  <a id=\"3.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nimg = plt.imread('../input/plant-pathology-2020-fgvc7/images/Train_500.jpg')\nprint(img.shape)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image processing and augmentation  <a id=\"3.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(image_size1, image_size2)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    #image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n    #image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n    #Make sure the image is still in [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\ntf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nif SPLIT_VALIDATION:\n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((valid_paths, valid_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)\n\ntrain_dataset_for_pred = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize results  <a id=\"3.5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_learning(history):\n    acc = history.history['categorical_accuracy']\n    loss = history.history['loss']\n    if SPLIT_VALIDATION: \n        val_acc = history.history['val_categorical_accuracy']\n        val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n    if SPLIT_VALIDATION: plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    plt.title('Accuracy')\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training Loss')\n    if SPLIT_VALIDATION: plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n    plt.title('Loss')\n    plt.legend()\n\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save predictions  <a id=\"3.6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_preds(model_name, pred_train, pred_test, pred_val=None):\n    \n    sub.loc[:, 'healthy':] = pred_test\n    filename_test = 'preds_' + model_name + '_test.csv'\n    sub.to_csv(filename_test, index=False)\n\n    train_labels_df.loc[:, 'healthy':] = pred_train\n    filename_train = 'preds_' + model_name + '_train.csv'\n    train_labels_df.to_csv(filename_train, index=False)\n\n    if SPLIT_VALIDATION:    \n        valid_labels_df.loc[:, 'healthy':] = pred_val\n        filename_val = 'preds_' + model_name + '_val.csv'\n        valid_labels_df.to_csv(filename_val, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning Rate  <a id=\"3.7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_START = 0.00001\nLR_MAX = 0.0001 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 15\nLR_SUSTAIN_EPOCHS = 3\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = [\n  tf.keras.callbacks.EarlyStopping(patience=10),\n  tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling  <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### EnfNetB7  <a id=\"4.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model_EffNetB7 = tf.keras.Sequential([ efn.EfficientNetB7( input_shape=(image_size1, image_size2, 3), \n                                                              weights='imagenet', \n                                                              include_top=False, \n                                                              pooling='avg'), \n                                                    L.Dense(4, activation='softmax')\n                                                    ])\n    \n    model_EffNetB7.compile(optimizer='adam', \n                           loss='categorical_crossentropy',\n                           metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_EffNetB7.fit(\n        train_dataset, \n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=lr_callback,\n        epochs=EPOCHS,\n        validation_data=valid_dataset if SPLIT_VALIDATION else None\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_EffNetB7 = model_EffNetB7.predict(train_dataset_for_pred)\npred_test_EffNetB7 = model_EffNetB7.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_EffNetB7 = model_EffNetB7.predict(valid_dataset)\n    save_preds('EffNetB7', pred_train_EffNetB7, pred_test_EffNetB7, pred_val_EffNetB7)\nelse:\n    save_preds('EffNetB7', pred_train_EffNetB7, pred_test_EffNetB7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet50V2  <a id=\"4.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n\nwith strategy.scope():\n    model_ResNet50V2 = tf.keras.Sequential([\n                    ResNet50V2(\n                        input_shape=(image_size1, image_size2, 3),\n                        weights='imagenet',\n                        include_top=False\n                    ),\n                    L.GlobalMaxPooling2D(),\n\n                    L.Dense(1024, activation='relu'),\n                    L.Dropout(0.5),\n                    L.BatchNormalization(),\n\n                    L.Dense(4, activation='softmax')\n                ])\n        \n    model_ResNet50V2.compile(\n        optimizer = 'adam',\n        loss = 'categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_ResNet50V2.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_ResNet50V2 = model_ResNet50V2.predict(train_dataset_for_pred)\npred_test_ResNet50V2 = model_ResNet50V2.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_ResNet50V2 = model_ResNet50V2.predict(valid_dataset)\n    save_preds('ResNet50V2', pred_train_ResNet50V2, pred_test_ResNet50V2, pred_val_ResNet50V2)\nelse:\n    save_preds('ResNet50V2', pred_train_ResNet50V2, pred_test_ResNet50V2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### InceptionResNetV2  <a id=\"4.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import InceptionResNetV2\n\nwith strategy.scope():\n    model_IncResNetV2 = tf.keras.Sequential([\n                InceptionResNetV2(\n                    input_shape=(image_size1, image_size2, 3),\n                    weights='imagenet',\n                    include_top=False\n                ),\n                L.GlobalMaxPooling2D(),\n\n                L.Dense(512, activation='relu'),\n                L.Dropout(0.5),\n                L.BatchNormalization(),\n\n                L.Dense(4, activation='softmax')\n            ])\n        \n    model_IncResNetV2.compile(\n        optimizer = 'adam',\n        loss = 'categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_IncResNetV2.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_IncResNetV2 = model_IncResNetV2.predict(train_dataset_for_pred)\npred_test_IncResNetV2 = model_IncResNetV2.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_IncResNetV2 = model_IncResNetV2.predict(valid_dataset)\n    save_preds('IncV3', pred_train_IncResNetV2, pred_test_IncResNetV2, pred_val_IncResNetV2)\nelse:\n    save_preds('IncV3', pred_train_IncResNetV2, pred_test_IncResNetV2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### InceptionV3  <a id=\"4.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n\nwith strategy.scope(): \n    model_IncV3 = tf.keras.Sequential([ InceptionV3( input_shape=(image_size1, image_size2, 3), \n                                                                       weights='imagenet', \n                                                                       include_top=False ), \n                                                    L.GlobalMaxPooling2D(), \n                                                    L.Dense(4, activation='softmax')\n                                                    ])\n    model_IncV3.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy', \n                  metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_IncV3.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_IncV3 = model_IncV3.predict(train_dataset_for_pred)\npred_test_IncV3 = model_IncV3.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_IncV3 = model_IncV3.predict(valid_dataset)\n    save_preds('IncV3', pred_train_IncV3, pred_test_IncV3, pred_val_IncV3)\nelse:\n    save_preds('IncV3', pred_train_IncV3, pred_test_IncV3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xception  <a id=\"4.5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import Xception\n\nwith strategy.scope(): \n    \n    model_Xcept = tf.keras.Sequential([Xception(input_shape=(image_size1, image_size2, 3),\n                                                            weights='imagenet',\n                                                            include_top=False),\n                                             L.GlobalAveragePooling2D(),\n                                             L.Dense(4, activation='softmax')\n                                             ])\n        \n    model_Xcept.compile(loss=\"categorical_crossentropy\", \n                        optimizer= 'adam', \n                        metrics=[\"categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_Xcept.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_Xcept = model_Xcept.predict(train_dataset_for_pred)\npred_test_Xcept = model_Xcept.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_Xcept = model_Xcept.predict(valid_dataset)\n    save_preds('Xcept', pred_train_Xcept, pred_test_Xcept, pred_val_Xcept)\nelse:\n    save_preds('Xcept', pred_train_Xcept, pred_test_Xcept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet152V2  <a id=\"4.6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import ResNet152V2\n\nwith strategy.scope():\n    model_ResNet152V2 = tf.keras.Sequential([ResNet152V2(input_shape=(image_size1, image_size2, 3),\n                                                            weights='imagenet',\n                                                            include_top=False),\n                                             L.GlobalAveragePooling2D(),\n                                             L.Dense(4, activation='softmax')\n                                             ])\n    \n    model_ResNet152V2.compile(loss=\"categorical_crossentropy\", \n                              optimizer= 'adam', \n                              metrics=[\"categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_ResNet152V2.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_ResNet152V2 = model_ResNet152V2.predict(train_dataset_for_pred)\npred_test_ResNet152V2 = model_ResNet152V2.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_ResNet152V2 = model_ResNet152V2.predict(valid_dataset)\n    save_preds('ResNet152V2', pred_train_ResNet152V2, pred_test_ResNet152V2, pred_val_ResNet152V2)\nelse:\n    save_preds('ResNet152V2', pred_train_ResNet152V2, pred_test_ResNet152V2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NASNet  <a id=\"4.7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.nasnet import NASNetLarge\nwith strategy.scope():    \n    model_NASNet = tf.keras.Sequential([NASNetLarge( input_shape=(image_size1, image_size2, 3), \n                                                                       weights='imagenet', \n                                                                       include_top=False ), \n                                                    L.GlobalMaxPooling2D(), \n                                                    L.Dense(4, activation='softmax')\n                                                    ])\n    model_NASNet.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy', \n                  metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_NASNet.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_NASNet = model_NASNet.predict(train_dataset_for_pred)\npred_test_NASNet = model_NASNet.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_NASNet = model_NASNet.predict(valid_dataset)\n    save_preds('NASNet', pred_train_NASNet, pred_test_NASNet, pred_val_NASNet)\nelse:\n    save_preds('NASNet', pred_train_NASNet, pred_test_NASNet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DenseNet201  <a id=\"4.8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import DenseNet201\n\nwith strategy.scope():\n    model_DenseNet201 = tf.keras.Sequential([DenseNet201(input_shape=(image_size1, image_size2, 3),\n                                                            weights='imagenet',\n                                                            include_top=False),\n                                             L.GlobalAveragePooling2D(),\n                                             L.Dense(4, activation='softmax')\n                                             ])\n    \n    model_DenseNet201.compile(loss=\"categorical_crossentropy\", \n                              optimizer= 'adam', \n                              metrics=[\"categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_DenseNet201.fit(\n    train_dataset, \n    epochs=EPOCHS, \n    callbacks=lr_callback,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=valid_dataset if SPLIT_VALIDATION else None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_DenseNet201 = model_DenseNet201.predict(train_dataset_for_pred)\npred_test_DenseNet201 = model_DenseNet201.predict(test_dataset)\n\nif SPLIT_VALIDATION:\n    pred_val_DenseNet201 = model_DenseNet201.predict(valid_dataset)\n    save_preds('DenseNet201', pred_train_DenseNet201, pred_test_DenseNet201, pred_val_DenseNet201)\nelse:\n    save_preds('DenseNet201', pred_train_DenseNet201, pred_test_DenseNet201)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu) # Clear TPU Memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking  <a id=\"5\"></a>\n1. Закачиваю тест и вал каждой модели\n1. Составляю ДФ трэйн и ДФ тест\n1. Тренирую модель\n1. Получаю предсказания\n1. Сохраняю предсказания"},{"metadata":{},"cell_type":"markdown","source":"### Avg  <a id=\"5.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_avg = (pred_test_EffNetB7 + pred_test_IncV3 + pred_test_Xcept) / 3\nsub.loc[:, 'healthy':] = preds_avg\nsub.to_csv('submission_avg_3model_NoSplit_800-533.csv', index=False)\nsub.head()\n# LB 0.98","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge meta model  <a id=\"5.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train = np.concatenate((pred_val_EffNetB7, pred_val_IncV3, pred_val_Xcept), axis=1)\npred_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = np.concatenate((pred_test_EffNetB7, pred_test_IncV3, pred_test_Xcept), axis=1)\npred_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nRidge = Ridge(alpha=1, random_state=241)\nRidge.fit(pred_train, valid_labels)\npredictions = Ridge.predict(pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = predictions\nsub.to_csv('submission_predict_ridge.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MLPClassifier meta model  <a id=\"5.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nMLP_clf = MLPClassifier(max_iter=400)\nMLP_clf.fit(pred_train, valid_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionMLP = MLP_clf.predict(pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = predictionMLP\nsub.to_csv('submission_3models_MLPReg.csv', index=False)\nsub.head()\n# LB 0.961 regression, 0.925 classifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Entropy  <a id=\"5.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ent1 = entropy(pred_test_EffNetB7, base=2, axis = 1)\nent2 = entropy(pred_test_IncV3, base=2, axis = 1)\nent3 = entropy(pred_test_DenseNet201, base=2, axis = 1)\nent4 = entropy(pred_test_Xcept, base=2, axis = 1)\nent5 = entropy(pred_test_ResNet152V2, base=2, axis = 1)\nentropies = np.array([ent1, ent2, ent3, ent4, ent5]).transpose()\nentropies.shape\n\nselected = np.argmin(entropies, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_size = len(selected)\nfor i in range(submission_size):\n    if selected[i] ==0:\n        sub.loc[i, 'healthy' : ] = sub1\n    elif selected[i] ==1:\n        sub.loc[i, 'healthy' : ] = sub2\n    elif selected[i] == 2:\n        sub.loc[i, 'healthy' : ] = sub3\n    elif selected[i] == 3:\n        sub.loc[i, 'healthy' : ] = sub4\n    elif selected[i] == 4:\n        sub.loc[i, 'healthy' : ] = sub5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}